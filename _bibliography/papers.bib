---
---
@misc{argoverse2019,
  author = {Chang*, Ming-Fang and Lambert*, John and Sangkloy*, Patsorn and Singh*, Jagjeet and Bak, Slawomir and Hartnett, Andrew and Wang, De and Carr, Peter and Lucey, Simon and Ramanan, Deva and Hays, James},
  title = {--coming soon--},
  howpublished = {To appear in CVPR},
  year = {2019},
  equal = {yes},
    note = {The first four authors contributed equally. },
   highlight = {Oral presentation}
}
@misc{cagan_wicv2019,
  author = {Jitkrittum*, Wittawat and Sangkloy*, Patsorn and Gondal, Muhammad Waleed
      and Raj, Amit and Hays, James and {Sch{\"o}lkopf}, Bernhard },
  title = {Generate Semantically Similar Images with Kernel Mean Matching},
  howpublished = {Women in Computer Vision Workshop, CVPR},
  year = {2019},
  equal = {yes},
  pdf = {cagan_wicv2019.pdf},
    note = {The first two authors contributed equally. },
   highlight = {Oral presentation}
}

@InProceedings{cagan_icml2019,
  author = {Jitkrittum*, Wittawat and Sangkloy*, Patsorn and Gondal, Muhammad Waleed
      and Raj, Amit and Hays, James and {Sch{\"o}lkopf}, Bernhard },
  title = {Kernel Mean Matching for Content Addressability of {GANs}},
  booktitle = {To appear in ICML},
  year = {2019},
  equal = {yes},
    note = {The first two authors contributed equally.},
}

@article{Sangkloy:2016:SDL:2897824.2925954,
 author = {Sangkloy, Patsorn and Burnell, Nathan and Ham, Cusuh and Hays, James},
 title = {The Sketchy Database: Learning to Retrieve Badly Drawn Bunnies},
 journal = {SIGGRAPH},
 abstract = {We present the Sketchy database, the first large-scale collection of sketch-photo pairs. We ask crowd workers to sketch particular photographic objects sampled from 125 categories and acquire 75,471 sketches of 12,500 objects. The Sketchy database gives us fine-grained associations between particular photos and sketches, and we use this to train cross-domain convolutional networks which embed sketches and photographs in a common feature space. We use our database as a benchmark for fine-grained retrieval and show that our learned representation significantly outperforms both hand-crafted features as well as deep features trained for sketch or photo classification. Beyond image retrieval, we believe the Sketchy database opens up new opportunities for sketch and image understanding and synthesis.},
 issue_date = {July 2016},
 volume = {35},
 number = {4},
 month = jul,
 year = {2016},
 issn = {0730-0301},
 pages = {119:1--119:12},
 articleno = {119},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2897824.2925954},
 html={http://sketchy.eye.gatech.edu},
 doi = {10.1145/2897824.2925954},
 acmid = {2925954},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {deep learning, image synthesis, siamese network, sketch-based image retrieval, triplet network},
}


@InProceedings{jitkrittum_kmod2018,
title = {Informative Features for Model Comparison},
author = {Wittawat Jitkrittum and Heishiro Kanagawa and
        Patsorn Sangkloy and James Hays and Bernhard Sch\"{o}lkopf and Arthur Gretton },
    booktitle = {NeurIPS},
year = {2018},
arxiv = {1810.11630},
abstract={Given two candidate models, and a set of target observations, we address the problem of measuring the relative goodness of fit of the two models. We propose two new statistical tests which are nonparametric, computationally efficient (runtime complexity is linear in the sample size), and interpretable. As a unique advantage, our tests can produce a set of examples (informative features) indicating the regions in the data domain where one model fits significantly better than the other. In a real-world problem of comparing GAN models, the test power of our new test matches that of the state-of-the-art test of relative goodness of fit, while being one order of magnitude faster.}
}


@InProceedings{Raj_2018_ECCV,
author = {Raj, Amit and Sangkloy, Patsorn and Chang, Huiwen and Lu, Jingwan and Ceylan, Duygu and Hays, James},
title = {SwapNet: Garment Transfer in Single View Images},
abstract={We present SwapNet, a framework to transfer garments across images of people with arbitrary body pose, shape, and clothing. Garment transfer is a challenging task that requires (i) disentangling the features of the clothing from the body pose and shape and (ii) realistic synthesis of the garment texture on the new body. We present a neural network architecture that tackles these sub-problems with two task-specific sub-networks. Since acquiring pairs of images showing the same clothing on different bodies is difficult, we propose a novel weakly-supervised approach that generates training pairs from a single image via data augmentation. We present the first fully automatic method for garment transfer in unconstrained images without solving the difficult 3D reconstruction problem. We demonstrate a variety of transfer results and highlight our advantages over traditional image-to-image and analogy pipelines.
},
booktitle = {ECCV},
month = {September},
year = {2018},
html = {http://www.eye.gatech.edu/swapnet/}
}

@InProceedings{Xian_2018_CVPR,
author = {Xian*, Wenqi and Sangkloy*, Patsorn and Agrawal, Varun and Raj, Amit and Lu, Jingwan and Fang, Chen and Yu, Fisher and Hays, James},
title = {TextureGAN: Controlling Deep Image Synthesis With Texture Patches},
abstract={In this paper, we investigate deep image synthesis guided by sketch, color, and texture. Previous image synthesis methods can be controlled by sketch and color strokes but we are the first to examine texture control. We allow a user to place a texture patch on a sketch at arbitrary locations and scales to control the desired output texture. Our generative network learns to synthesize objects consistent with these texture suggestions. To achieve this, we develop a local texture loss in addition to adversarial and content loss to train the generative network. We conduct experiments using sketches generated from real images and textures sampled from a separate texture database and results show that our proposed algorithm is able to generate plausible images that are faithful to user controls. Ablation studies show that our proposed pipeline can generate more realistic images than adapting existing methods directly.
},
booktitle = {CVPR},
month = {June},
year = {2018},
equal = {yes},
highlight = {Spotlight presentation 6.8%},
html = {http://texturegan.eye.gatech.edu/}
}

@InProceedings{Sangkloy_2017_CVPR,
author = {Sangkloy, Patsorn and Lu, Jingwan and Fang, Chen and Yu, Fisher and Hays, James},
title = {Scribbler: Controlling Deep Image Synthesis With Sketch and Color},
abstract={Recently, there have been several promising methods to generate realistic imagery from deep convolutional networks. These methods sidestep the traditional computer graphics rendering pipeline and instead generate imagery at the pixel level by learning from large collections of photos (e.g. faces or bedrooms). However, these methods are of limited utility because it is difficult for a user to control what the network produces. In this paper, we propose a deep adverserial image synthesis architecture that is conditioned on coarse sketches and sparse color strokes to generate realistic cars, bedrooms, or faces. We demonstrate a sketch based image synthesis system which allows users to 'scribble' over the sketch to indicate preferred color for objects. Our network can then generate convincing images that satisfy both the color and the sketch constraints of user. The network is feed-forward which allows users to see the effect of their edits in real time. We compare to recent work on sketch to image synthesis and show that our approach can generate more realistic, more diverse, and more controllable outputs. The architecture is also effective at user-guided colorization of grayscale images.},
booktitle = {CVPR},
month = {July},
year = {2017},
html = {http://scribbler.eye.gatech.edu},
highlight = {Feature in <a href="https://www.youtube.com/watch?v=seBbuYiBXSc" >Adobe Max 2017</a>}
}

@article{Papoutsaki_Sangkloy_Laskey_Daskalova_Huang_Hays_2016, html={https://webgazer.cs.brown.edu/},place={Country unknown/Code not available}, title={WebGazer: Scalable Webcam Eye Tracking Using User Interactions}, url={http://par.nsf.gov/biblio/10024076}, abstract={We introduce WebGazer, an online eye tracker that uses common webcams already present in laptops and mobile devices to infer the eye-gaze locations of web visitors on a page in real time. The eye tracking model self-calibrates by watching web visitors interact with the web page and trains a mapping between features of the eye and positions on the screen. This approach aims to provide a natural experience to everyday users that is not restricted to laboratories and highly controlled user studies. WebGazer has two key components: a pupil detector that can be combined with any eye detection library, and a gaze estimator using regression analysis informed by user interactions. We perform a large remote online study and a small in-person study to evaluate WebGazer. The findings show that WebGazer can learn from user interactions and that its accuracy is sufficient for approximating the userâ€™s gaze. As part of this paper, we release the first eye tracking library that can be easily integrated in any website for real-time gaze interactions, usability studies, or web research.}, journal={IJCAI}, author={Papoutsaki, Alexandra and Sangkloy, Patsorn and Laskey, James and Daskalova, Nediyana and Huang, Jeff and Hays, James}, year={2016}, month={Jan}}

@article{DBLP:journals/corr/abs-1801-07388,
  author    = {Daniel Castro and
               Steven Hickson and
               Patsorn Sangkloy and
               Bhavishya Mittal and
               Sean Dai and
               James Hays and
               Irfan A. Essa},
  title     = {Let's Dance: Learning From Online Dance Videos},
  journal   = {CoRR},
  volume    = {abs/1801.07388},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.07388},
  archivePrefix = {arXiv},
  eprint    = {1801.07388},
  abstract  = {In recent years, deep neural network approaches have naturally extended to the video domain, in their simplest case by aggregating per-frame classifications as a baseline for action recognition. A majority of the work in this area extends from the imaging domain, leading to visual-feature heavy approaches on temporal data. To address this issue we introduce ``Let's Dance'', a 1000 video dataset (and growing) comprised of 10 visually overlapping dance categories that require motion for their classification. We stress the important of human motion as a key distinguisher in our work given that, as we show in this work, visual information is not sufficient to classify motion-heavy categories. We compare our datasets' performance using imaging techniques with UCF-101 and demonstrate this inherent difficulty. We present a comparison of numerous state-of-the-art techniques on our dataset using three different representations (video, optical flow and multi-person pose data) in order to analyze these approaches. We discuss the motion parameterization of each of them and their value in learning to categorize online dance videos. Lastly, we release this dataset (and its three representations) for the research community to use.},
  timestamp = {Mon, 13 Aug 2018 16:48:38 +0200},
  html      = {https://www.cc.gatech.edu/cpl/projects/dance/},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1801-07388},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
